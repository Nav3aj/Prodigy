{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2373fe5b",
   "metadata": {},
   "source": [
    "# Task-01: Text Generation with GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eafa85",
   "metadata": {},
   "source": [
    "## Check Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "missing = [lib for lib in (\"transformers\",\"datasets\") if importlib.util.find_spec(lib) is None]\n",
    "print(\"Missing:\", missing if missing else \"All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110eb861",
   "metadata": {},
   "source": [
    "## Import Libraries and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1540b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fbb49",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:1%]')\n",
    "def tokenize_fun(examples): return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "tokenized = dataset.map(tokenize_fun, batched=True)\n",
    "tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63990cd7",
   "metadata": {},
   "source": [
    "##  Fine-Tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=200,\n",
    "    save_total_limit=1\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae1c0d",
   "metadata": {},
   "source": [
    "##  Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35639ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c02465",
   "metadata": {},
   "source": [
    "Created by Navraj Amgai"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
